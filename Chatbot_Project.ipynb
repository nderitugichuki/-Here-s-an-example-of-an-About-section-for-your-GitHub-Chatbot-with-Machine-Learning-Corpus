{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP2BWEAZMM4E"
      },
      "source": [
        "Importing Corpus and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UljUuGr84xQV"
      },
      "outputs": [],
      "source": [
        "#import the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cWUUFtXF3uO",
        "outputId": "cb554d1c-0297-4ff5-9005-d503ce112360"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#importing and raeding the corpus\n",
        "f=open('MLchatbot.txt','r')\n",
        "raw_doc=f.read()\n",
        "raw_doc=raw_doc.lower() #converts text to lowercase\n",
        "nltk.download('punkt') #using the punkt tokenizer\n",
        "nltk.download('wordnet') #using the wordnet dictionary\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc)#converts doc to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw_doc)#converts doc to list of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jXpchlnL7VX",
        "outputId": "1c50cf57-7ca5-4149-81ae-f1db7924a4b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['machine learning (ml) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.',\n",
              " '[1] quick progress in the field of deep learning, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance.']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Sentence tokens\n",
        "sent_tokens[:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIbMvE7aMGHe",
        "outputId": "aaf32054-9853-4d10-99d3-45095e03a9bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['machine', 'learning']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokens[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTJa6GzKMUJ2"
      },
      "source": [
        "Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F21ZfGaCMW32"
      },
      "outputs": [],
      "source": [
        "lemmer=nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "  remove_punct_dict=dict((ord(punct),None) for punct in string.punctuation)\n",
        "  def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouKJTIPxQY1n"
      },
      "source": [
        "Defining greeting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8HA6195QcFd"
      },
      "outputs": [],
      "source": [
        "GREET_INPUTS=(\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\")\n",
        "GREET_RESPONSES=[\"hi\",\"hey\",\"*nods*\",\"hi there\",\"hello\",\"I am glad! You are talking to me\"]\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBMO0PYQ04Q"
      },
      "source": [
        "Response Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cla2OfObXoZ1"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkL0phKMYFRh"
      },
      "outputs": [],
      "source": [
        "def response(user_response):\n",
        "  robo1_response=''\n",
        "  TfidfVec=TfidfVector(tokenizer=LemNormalize,stop_words='english')\n",
        "  tfidf=TfidfVec.fit_transform(sent_tokens)\n",
        "  vals=cosine_similarity(tfidf[-1],tfidf)\n",
        "  idx=vals.argsort()[0][-2]\n",
        "  flat=vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf=flat[-2]\n",
        "  if(req_tfidf==0):\n",
        "    robo1_response=robo1_response+\"I am sorry! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response=robo1_response+sent_tokens[idx]\n",
        "    return robo1_response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8atkBVd2dGY9"
      },
      "source": [
        "Defining conversation start/end protocols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3rJZzf8h-Jb",
        "outputId": "356b5268-047e-4c6e-f404-de24d8eb80f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: My name is BOT. Let's have a conversation! Also, if you want to exit any time, just type Bye!\n"
          ]
        }
      ],
      "source": [
        "flag=True\n",
        "print(\"BOT: My name is BOT. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n",
        "while(flag==True):\n",
        "  user_response=input()\n",
        "  user_response=user_response.lower() # Call the lower() method to get the lowercase string\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response=='thanks' or user_response=='thank you'):\n",
        "      flag=False\n",
        "      print(\"BOT: You are welcome..\")\n",
        "    else:\n",
        "        if(greet(user_response)!=None):\n",
        "          print(\"BOT: \"+greet(user_response))\n",
        "        else:\n",
        "          sent_tokens.append(user_response)\n",
        "          word_tokens=word_tokens+nltk.word_tokenize(user_response)\n",
        "          final_words=list(set(word_tokens))\n",
        "          print(\"BOT: \",end=\"\")\n",
        "          print(response(user_response=user_response))\n",
        "          sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag=False\n",
        "    print(\"BOT: Goodbye! Take care <3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWNy7FwXgWz5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}